{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOTUS Evaluations Demo\n",
    "\n",
    "This notebook demonstrates the usage of LOTUS's AI evaluations subpackage (`lotus.evals`). The package provides two main evaluation methods:\n",
    "\n",
    "1. **`llm_as_judge`** - Use an LLM to evaluate/score individual responses\n",
    "2. **`pairwise_judge`** - Use an LLM to compare two responses and determine which is better\n",
    "\n",
    "Both methods are available as pandas DataFrame accessors, making them easy to integrate into data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and configure the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "import lotus\n",
    "from lotus.models import LM\n",
    "from lotus.types import ReasoningStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the language model\n",
    "# You can use any model supported by LiteLLM (OpenAI, Anthropic, etc.)\n",
    "lm = LM(model=\"gpt-4o-mini\")\n",
    "lotus.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. LLM as Judge - Basic Usage\n",
    "\n",
    "The `llm_as_judge` accessor allows you to use an LLM to evaluate responses in your DataFrame. This is useful for:\n",
    "- Grading student answers\n",
    "- Evaluating model outputs\n",
    "- Quality assessment of generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: student responses to ML questions\n",
    "data = {\n",
    "    \"student_id\": [1, 2, 3, 4],\n",
    "    \"question\": [\n",
    "        \"Explain the difference between supervised and unsupervised learning\",\n",
    "        \"What is the purpose of cross-validation in machine learning?\",\n",
    "        \"Describe how gradient descent works\",\n",
    "        \"What are the advantages of ensemble methods?\",\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Supervised learning uses labeled data to train models, while unsupervised learning finds patterns in unlabeled data. For example, classification is supervised, clustering is unsupervised.\",\n",
    "        \"Gradient descent is an optimization algorithm that minimizes cost functions.\",  # Wrong answer!\n",
    "        \"Cross-validation helps assess model performance by splitting data into training and validation sets multiple times to get a better estimate of how the model generalizes.\",\n",
    "        \"Ensemble methods combine multiple models to improve performance. They reduce overfitting and variance, often leading to better generalization than individual models.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic evaluation: score answers on a 1-10 scale\n",
    "judge_instruction = (\n",
    "    \"Rate the accuracy and completeness of this {answer} to the {question} \"\n",
    "    \"on a scale of 1-10, where 10 is excellent. Only output the score.\"\n",
    ")\n",
    "\n",
    "results = df.llm_as_judge(\n",
    "    judge_instruction=judge_instruction,\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_judge_0` column contains the evaluation results. Notice that student 2 likely received a lower score because their answer was about gradient descent, not cross-validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Multiple Trials for Robustness\n",
    "\n",
    "LLM evaluations can be noisy. Running multiple trials helps assess consistency and get more robust scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3 independent trials\n",
    "results_multi = df.llm_as_judge(\n",
    "    judge_instruction=judge_instruction,\n",
    "    n_trials=3,\n",
    ")\n",
    "\n",
    "# View all trial results\n",
    "results_multi[[\"student_id\", \"question\", \"_judge_0\", \"_judge_1\", \"_judge_2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average score across trials\n",
    "judge_cols = [col for col in results_multi.columns if col.startswith(\"_judge\")]\n",
    "results_multi[\"avg_score\"] = results_multi[judge_cols].astype(float).mean(axis=1)\n",
    "results_multi[[\"student_id\", \"question\", \"avg_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Structured Output with Response Format\n",
    "\n",
    "For more detailed evaluations, you can use Pydantic models to define a structured response format. This ensures consistent, parseable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structured evaluation schema\n",
    "class EvaluationScore(BaseModel):\n",
    "    score: int = Field(description=\"Score from 1-10\")\n",
    "    reasoning: str = Field(description=\"Detailed reasoning for the score\")\n",
    "    strengths: list[str] = Field(description=\"Key strengths of the answer\")\n",
    "    improvements: list[str] = Field(description=\"Areas for improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with structured output\n",
    "results_structured = df.llm_as_judge(\n",
    "    judge_instruction=\"Evaluate the student {answer} for the {question}\",\n",
    "    response_format=EvaluationScore,\n",
    "    suffix=\"_eval\",\n",
    ")\n",
    "\n",
    "results_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access structured evaluation data\n",
    "for idx, row in results_structured.iterrows():\n",
    "    eval_result = row[\"_eval_0\"]\n",
    "    print(f\"\\nStudent {row['student_id']}:\")\n",
    "    print(f\"  Score: {eval_result.score}/10\")\n",
    "    print(f\"  Reasoning: {eval_result.reasoning}\")\n",
    "    print(f\"  Strengths: {eval_result.strengths}\")\n",
    "    print(f\"  Improvements: {eval_result.improvements}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Custom System Prompts\n",
    "\n",
    "You can customize the system prompt to set up the judge with specific expertise or evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom system prompt for a strict ML expert\n",
    "custom_system_prompt = (\n",
    "    \"You are a senior machine learning professor grading PhD qualifying exams. \"\n",
    "    \"You have extremely high standards and expect precise, technically accurate answers. \"\n",
    "    \"Penalize heavily for any inaccuracies or missing key concepts.\"\n",
    ")\n",
    "\n",
    "results_strict = df.llm_as_judge(\n",
    "    judge_instruction=judge_instruction,\n",
    "    system_prompt=custom_system_prompt,\n",
    ")\n",
    "\n",
    "results_strict[[\"student_id\", \"question\", \"_judge_0\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Pairwise Comparison\n",
    "\n",
    "The `pairwise_judge` accessor compares two responses side-by-side. This is especially useful for:\n",
    "- A/B testing model outputs\n",
    "- Comparing baseline vs. improved responses\n",
    "- Preference ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: comparing two model outputs\n",
    "comparison_data = {\n",
    "    \"prompt\": [\n",
    "        \"Write a one-sentence summary of the benefits of regular exercise.\",\n",
    "        \"Explain the difference between supervised and unsupervised learning in one sentence.\",\n",
    "        \"Suggest a polite email subject line to schedule a 1:1 meeting.\",\n",
    "    ],\n",
    "    \"model_a\": [\n",
    "        \"Regular exercise improves physical health and mental well-being by boosting energy, mood, and resilience.\",\n",
    "        \"Supervised learning uses labeled data to learn mappings, while unsupervised learning finds patterns without labels.\",\n",
    "        \"Meeting request.\",\n",
    "    ],\n",
    "    \"model_b\": [\n",
    "        \"Exercise is good.\",\n",
    "        \"Supervised learning and unsupervised learning are both machine learning approaches.\",\n",
    "        \"Requesting a 1:1: finding time to connect next week?\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df_compare = pd.DataFrame(comparison_data)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model outputs\n",
    "pairwise_instruction = (\n",
    "    \"Given the prompt {prompt}, compare the two responses.\\n\"\n",
    "    \"Output only 'A' or 'B' or 'Tie' if the responses are equally good.\"\n",
    ")\n",
    "\n",
    "pairwise_results = df_compare.pairwise_judge(\n",
    "    col1=\"model_a\",\n",
    "    col2=\"model_b\",\n",
    "    judge_instruction=pairwise_instruction,\n",
    ")\n",
    "\n",
    "pairwise_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Position Bias Mitigation with `permute_cols`\n",
    "\n",
    "LLMs can exhibit position bias (e.g., preferring the first option). The `permute_cols` option runs evaluations with both orderings to mitigate this bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pairwise evaluation with position permutation\n",
    "# Note: n_trials must be even when permute_cols=True\n",
    "pairwise_permuted = df_compare.pairwise_judge(\n",
    "    col1=\"model_a\",\n",
    "    col2=\"model_b\",\n",
    "    judge_instruction=pairwise_instruction,\n",
    "    n_trials=4,  # 2 trials for each ordering\n",
    "    permute_cols=True,\n",
    ")\n",
    "\n",
    "# View all trial results\n",
    "judge_cols = [col for col in pairwise_permuted.columns if \"_judge\" in col]\n",
    "pairwise_permuted[[\"prompt\"] + judge_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Structured Pairwise Comparison\n",
    "\n",
    "Combine pairwise comparison with structured outputs for detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define structured comparison schema\n",
    "class ComparisonResult(BaseModel):\n",
    "    winner: str = Field(description=\"'A', 'B', or 'Tie'\")\n",
    "    reasoning: str = Field(description=\"Why this response is better\")\n",
    "    score_a: int = Field(description=\"Score for response A (1-10)\")\n",
    "    score_b: int = Field(description=\"Score for response B (1-10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured pairwise comparison\n",
    "pairwise_structured = df_compare.pairwise_judge(\n",
    "    col1=\"model_a\",\n",
    "    col2=\"model_b\",\n",
    "    judge_instruction=(\n",
    "        \"Given the prompt {prompt}, compare the two responses. \"\n",
    "        \"Determine which is better and explain why.\"\n",
    "    ),\n",
    "    response_format=ComparisonResult,\n",
    ")\n",
    "\n",
    "pairwise_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed comparison results\n",
    "for idx, row in pairwise_structured.iterrows():\n",
    "    result = row[\"_judge_0\"]\n",
    "    print(f\"\\nPrompt: {row['prompt'][:50]}...\")\n",
    "    print(f\"  Winner: {result.winner}\")\n",
    "    print(f\"  Scores: A={result.score_a}, B={result.score_b}\")\n",
    "    print(f\"  Reasoning: {result.reasoning}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Chain-of-Thought Reasoning\n",
    "\n",
    "For complex evaluations, you can enable chain-of-thought (CoT) reasoning to get the model to think step-by-step before giving a final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable zero-shot chain-of-thought reasoning\n",
    "results_cot = df.llm_as_judge(\n",
    "    judge_instruction=judge_instruction,\n",
    "    strategy=ReasoningStrategy.ZS_COT,\n",
    "    return_explanations=True,  # Capture the CoT reasoning\n",
    ")\n",
    "\n",
    "results_cot[[\"student_id\", \"_judge_0\", \"explanation_judge_0\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the chain-of-thought reasoning\n",
    "for idx, row in results_cot.iterrows():\n",
    "    print(f\"\\nStudent {row['student_id']}:\")\n",
    "    print(f\"  Score: {row['_judge_0']}\")\n",
    "    print(f\"  Reasoning: {row['explanation_judge_0']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Few-Shot Learning with Examples\n",
    "\n",
    "Provide example evaluations to guide the judge's behavior and calibrate scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create examples DataFrame with expected answers\n",
    "examples = pd.DataFrame({\n",
    "    \"question\": [\n",
    "        \"What is machine learning?\",\n",
    "        \"Define overfitting.\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Machine learning is a subset of AI that enables systems to learn from data without being explicitly programmed.\",\n",
    "        \"Overfitting is bad.\"\n",
    "    ],\n",
    "    \"Answer\": [  # Required column with expected judge output\n",
    "        \"9\",  # Good answer\n",
    "        \"2\",  # Poor answer\n",
    "    ]\n",
    "})\n",
    "\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with few-shot examples\n",
    "results_fewshot = df.llm_as_judge(\n",
    "    judge_instruction=judge_instruction,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "results_fewshot[[\"student_id\", \"question\", \"_judge_0\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Accessing Raw Outputs\n",
    "\n",
    "For debugging or advanced analysis, you can access the raw model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw outputs for debugging\n",
    "results_raw = df.llm_as_judge(\n",
    "    judge_instruction=judge_instruction,\n",
    "    return_raw_outputs=True,\n",
    ")\n",
    "\n",
    "results_raw[[\"student_id\", \"_judge_0\", \"raw_output_judge_0\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "The LOTUS evaluations subpackage provides:\n",
    "\n",
    "| Feature | `llm_as_judge` | `pairwise_judge` |\n",
    "|---------|----------------|------------------|\n",
    "| **Use Case** | Score individual responses | Compare two responses |\n",
    "| **Multiple Trials** | `n_trials` parameter | `n_trials` parameter |\n",
    "| **Structured Output** | `response_format` (Pydantic) | `response_format` (Pydantic) |\n",
    "| **Position Bias Mitigation** | N/A | `permute_cols=True` |\n",
    "| **Chain-of-Thought** | `strategy=ReasoningStrategy.ZS_COT` | `strategy=ReasoningStrategy.ZS_COT` |\n",
    "| **Few-Shot Examples** | `examples` DataFrame | `examples` DataFrame |\n",
    "| **Custom System Prompt** | `system_prompt` | `system_prompt` |\n",
    "\n",
    "These tools integrate seamlessly with pandas DataFrames, making it easy to evaluate large datasets of model outputs or human responses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
